#
---
name: ((deployment_name))

releases:
  # can use file:// URLs - then SHA1 not required
- name: bosh
  # https://bosh.io/releases/github.com/cloudfoundry/bosh?all=1
  url: https://bosh.io/d/github.com/cloudfoundry/bosh?v=263.3.0
  sha1: 39a0c2a4bb478a01861dc2ff7cce35b1be79a34c
- name: bosh-aws-cpi
  # https://bosh.io/releases/github.com/cloudfoundry-incubator/bosh-aws-cpi-release?all=1 - this may, eventually change to not-incubator
  url: https://bosh.io/d/github.com/cloudfoundry-incubator/bosh-aws-cpi-release?v=66
  sha1: 117d5518f1b1de8937c163244de8db45ad8ce1a9

resource_pools:
- name: bosh_vm
  network: cf_director
  env:
    bosh:
      # c1oudc0w
      password: $6$GiQp7XJ3AYAnpUpZ$zGUSoI6OATmb4ff0k60ZFIk/pMNypGxqMoihspINUAS8Fk6p0Dz/PIMzFfguZIbLSeJPB5PkNMCNzkO4yJqp8/
  stemcell:
    name: bosh-aws-xen-hvm-ubuntu-trusty-go_agent
    # https://bosh.io/stemcells/
    # Sometimes the version stays the same but the hash changes
    #
    # The following error occurs when a normal stemcell is used:
    # creating stemcell (bosh-aws-xen-hvm-ubuntu-trusty-go_agent 3363.20):
    #  CPI 'create_stemcell' method responded with error: CmdError{"type":"Unknown","message":"Connection refused - Connection refused - connect(2) for \"169.254.169.254\" port 80 (169.254.169.254:80)","ok_to_retry":false}
    #
    # Despite not having 'light' in the URL, this always seems to download the 'light' version' (2017/05/15)
    #
    # If the stemcell can't be reached from the bosh.io URL we can use the direct AWS S3 URL
    #url: https://s3.amazonaws.com/bosh-aws-light-stemcells/light-bosh-stemcell-3431.13-aws-xen-hvm-ubuntu-trusty-go_agent.tgz
    url: https://bosh.io/d/stemcells/bosh-aws-xen-hvm-ubuntu-trusty-go_agent?v=3445.11
    sha1: e978f09e8e706fbdd8083c52ecd382efad921071
  cloud_properties:
    instance_type: t2.medium
    ephemeral_disk: {size: 50_000, type: gp2}
    availability_zone: ((aws_availability_zone1))
    security_groups: [((director_instance_security_group))]
    iam_instance_profile: ((director_instance_profile))
    elbs: [((director_elb))]

disk_pools:
- name: disks
  disk_size: 20_000
  cloud_properties: {type: gp2}

networks:
- name: cf_director
  type: manual
  subnets:
  - range: ((director_az1_cidr))
    gateway: ((director_az1_default_route))
    dns: [((dns_ip))]
    reserved: ["((director_az1_reserved_start)) - ((director_az1_reserved_stop))"]
    static: ["((director_az1_static_start)) - ((director_az1_static_stop))"]
    cloud_properties:
      subnet: ((director_az1_subnet))
#- name: public
#  type: vip

jobs:
- name: ((deployment_name))-boshlite
  instances: 1
  templates:
  - {name: nats, release: bosh}
  - {name: postgres-9.4, release: bosh}
  - {name: blobstore, release: bosh}
  - {name: director, release: bosh}
  - {name: health_monitor, release: bosh}
  - {name: registry, release: bosh}
  - {name: aws_cpi, release: bosh-aws-cpi}
  resource_pool: bosh_vm
  persistent_disk_pool: disks
  networks:
  - name: cf_director
    static_ips: [((bosh_lite_private_ip))]
    default: [dns, gateway]
  #- name: public
  #  static_ips: [((bosh_lite_public_ip))]                                                          # BOSH_ELASTIC_IP
  properties:
    agent: {mbus: "nats://nats:((nats_password))@((bosh_lite_private_ip)):4222"}
    aws: &aws
      # Health Monitor doesn't support instance access profiles, but it may do one day....
      credentials_source: env_or_profile
      access_key_id: ((bosh_aws_access_key_id))
      secret_access_key: ((bosh_aws_secret_access_key))
      default_key_name: ((bosh_ssh_key_name))
      region: ((aws_region))
      default_security_groups: [((director_instance_security_group))]
      # All VMs need to be able to write to the blob store to create logs!
      default_iam_instance_profile: ((blobstore_bucket_access_instance_profile))
    blobstore:
      #address: ((bosh_lite_private_ip))
      provider: s3
      s3_region: ((aws_region))
      credentials_source: env_or_profile
      bucket_name: ((blobstore_bucket))
      #port: 25250
      #provider: dav
      director:
        user: director
        password: ((director_blobstore_password))
      agent:
        user: agent
        password: ((agent_blobstore_password))
    compiled_package_cache:
      provider: s3
      options:
        s3_region: ((aws_region))
        credentials_source: env_or_profile
        bucket_name: ((compiled_package_cache_bucket))
    default_ssh_options:
      gateway_host: ((director_dns))
    director:
      address: 127.0.0.1
      name: ((deployment_name))
      backup_destination:
        provider: s3
        options:
          s3_region: ((aws_region))
          credentials_source: env_or_profile
          bucket_name: ((director_backup_bucket))
      #generate_vm_passwords: true
      # https://bosh.io/docs/remove-dev-tools.html
      #remove_dev_tools: true
      db: *db
      cpi_job: aws_cpi
      enable_dedicated_status_worker: true
      enable_nats_delivered_templates: true
      enable_post_deploy: true
      enable_snapshots: true
      # To/from Director
      #encryption: true
      max_threads: 10
      ssl:
        key: ((director_key))
        cert: ((director_crt))
      user_management:
        provider: local
        local:
          users:
          - {name: director, password: ((director_password))}
          - {name: hm, password: ((hm_password))}
    hm:
      cloud_watch_enabled: true
      director_account:
        user: hm
        password: ((hm_password))
      resurrector_enabled: true
    nats:
      address: 127.0.0.1
      user: nats
      password: ((nats_password))
    ntp: &ntp [0.pool.ntp.org, 1.pool.ntp.org]
    postgres: &db
      listen_address: 127.0.0.1
      host: 127.0.0.1
      user: postgres
      password: ((postgres_password))
      database: bosh
      adapter: postgres
    registry:
      address: ((bosh_lite_private_ip))
      host: ((bosh_lite_private_ip))
      db: *db
      http:
        user: admin
        password: ((http_password))
        port: 25777
      username: admin
      password: ((registry_password))
      port: 25777

properties:

addons:

cloud_provider:
  mbus: "https://mbus:((mbus_password))@((director_dns)):6868"
  ssh_tunnel:
    host: ((director_dns))
    port: 22
    user: vcap
    private_key: ((bosh_ssh_key_file))
  template: {name: aws_cpi, release: bosh-aws-cpi}
  properties:
    aws:
      access_key_id: ((bosh_aws_access_key_id))
      secret_access_key: ((bosh_aws_secret_access_key))
      default_key_name: ((bosh_ssh_key_name))
      region: ((aws_region))
      default_security_groups: [((director_instance_security_group))]
      # All VMs need to be able to write to the blob store to create logs!
      default_iam_instance_profile: ((blobstore_bucket_access_instance_profile))
    agent: {mbus: "https://mbus:((mbus_password))@0.0.0.0:6868"}
    blobstore: {provider: local, path: /var/vcap/micro_bosh/data/cache}
    ntp: *ntp
