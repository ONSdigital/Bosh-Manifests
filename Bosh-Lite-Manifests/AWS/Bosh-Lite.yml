#
---
name: ((deployment_name))

# Pull out these values to enrich the bosh-config.sh
metadata:
  director_ca: ((default_ca.certificate))
  director_secret: ((director_password))

releases:
- name: bosh
  # https://bosh.io/releases/github.com/cloudfoundry/bosh?all=1
  url: https://bosh.io/d/github.com/cloudfoundry/bosh?v=264.6.0
  sha1: 11ffc6012167e7147ddfc1a4389c9fce65de37c0
- name: bosh-aws-cpi
  # https://bosh.io/releases/github.com/cloudfoundry-incubator/bosh-aws-cpi-release?all=1 - this may, eventually change to not-incubator
  url: https://bosh.io/d/github.com/cloudfoundry-incubator/bosh-aws-cpi-release?v=69
  sha1: 8abe70219244896ea6f7208fc01f2eac56179170
- name: postgresql-databases
  url: ((postgresql_databases_url))
  version: latest

resource_pools:
- name: bosh_vm
  network: cf_director
  env:
    bosh:
      # The following should work to generate a new password:
      # echo "$PASSWORD $SALT" | perl -ane 'print crypt($F[0],sprintf(q($6$%s$),$F[1])).qq(\n)'
      #
      # c1oudc0w
      password: $6$GiQp7XJ3AYAnpUpZ$zGUSoI6OATmb4ff0k60ZFIk/pMNypGxqMoihspINUAS8Fk6p0Dz/PIMzFfguZIbLSeJPB5PkNMCNzkO4yJqp8/
      mbus:
        cert: ((mbus_bootstrap_ssl))
  stemcell:
    name: bosh-aws-xen-hvm-ubuntu-trusty-go_agent
    # Sometimes, there are problems connecting disks. This seems to come and go between various releases.  Some AWS regions seem to suffer more than others
    #
    # [CLI] 2017/10/16 09:49:42 ERROR - Deploying: Creating instance 'clementine-boshlite/0': Updating instance disks: Updating disks: Deploying disk: Mounting disk: Sending 'get_task' to the agent: Agent responded with error: Action Failed get_task: Task f4b307dc-b987-4d0f-6592-0cad27bd3586 result: Persistent disk with volume id 'vol-0fc18013ccfad16dd' could not be found
    #
    # Sometimes the version stays the same but the hash changes
    #
    # The following error occurs when a normal stemcell is used:
    # creating stemcell (bosh-aws-xen-hvm-ubuntu-trusty-go_agent 3363.20):
    #  CPI 'create_stemcell' method responded with error: CmdError{"type":"Unknown","message":"Connection refused - Connection refused - connect(2) for \"169.254.169.254\" port 80 (169.254.169.254:80)","ok_to_retry":false}
    #
    # Despite not having 'light' in the URL, this always seems to download the 'light' version' (2017/05/15)
    #
    # If the stemcell can't be reached from the bosh.io URL we can use the direct AWS S3 URL
    # url: https://s3.amazonaws.com/bosh-aws-light-stemcells/light-bosh-stemcell-3431.13-aws-xen-hvm-ubuntu-trusty-go_agent.tgz
    #
    # https://bosh.io/stemcells/
    # https://bosh.io/stemcells/bosh-aws-xen-hvm-ubuntu-trusty-go_agent
    #
    url: https://bosh.io/d/stemcells/bosh-aws-xen-hvm-ubuntu-trusty-go_agent?v=3468.17
    sha1: c09040e9e0fcef6dffaece68f8d4173066e4f458

  cloud_properties:
    instance_type: m4.large
    ephemeral_disk: {size: 20_000, type: gp2}
    availability_zone: ((aws_availability_zone1))
    security_groups: [((director_instance_security_group))]
    iam_instance_profile: ((director_instance_profile))
    elbs: [((director_elb))]

disk_pools:
- name: tiny_disk
  disk_size: 1_000
  cloud_properties: {type: gp2}

networks:
- name: cf_director
  type: manual
  subnets:
  - range: ((director_az1_cidr))
    gateway: ((director_az1_default_route))
    dns: [((dns_ip))]
    reserved: ["((director_az1_reserved_start)) - ((director_az1_reserved_stop))"]
    static: ["((director_az1_static_start)) - ((director_az1_static_stop))"]
    cloud_properties:
      subnet: ((director_az1_subnet))

jobs:
- name: ((deployment_name))-boshlite
  instances: 1
  templates:
  - {name: nats, release: bosh}
  - {name: blobstore, release: bosh}
  - {name: director, release: bosh}
  - {name: health_monitor, release: bosh}
  - {name: registry, release: bosh}
  - {name: aws_cpi, release: bosh-aws-cpi}
  - {name: database-creation, release: postgresql-databases}
  resource_pool: bosh_vm
  # When AWS is under load, we seem to get this error:
  #
  # Agent responded with error: Action Failed get_task: Task 49126ffb-2ba6-47b5-7507-17db35da90ce result: Persistent disk with volume id 'vol-0d593817628ad380a' could not be found
  #
  # Bosh only uses its disk for its Director logs
  persistent_disk_pool: tiny_disk
  networks:
  - name: cf_director
    static_ips: [((bosh_lite_private_ip))]
    default: [dns, gateway]
  properties:
    agent: {mbus: "nats://nats:((nats_password))@((bosh_lite_private_ip)):4222"}
    aws: &aws
      # Health Monitor doesn't support instance access profiles, but it may do one day....
      # credentials_source: env_or_profile
      credentials_source: static
      access_key_id: ((bosh_aws_access_key_id))
      secret_access_key: ((bosh_aws_secret_access_key))
      default_key_name: ((bosh_ssh_key_name))
      region: ((aws_region))
      default_security_groups: [((director_instance_security_group))]
      # All VMs need to be able to write to the blob store to create logs!
      default_iam_instance_profile: ((blobstore_bucket_access_instance_profile))
    blobstore:
      #address: ((bosh_lite_private_ip))
      provider: s3
      s3_region: ((aws_region))
      credentials_source: env_or_profile
      bucket_name: ((blobstore_bucket))
      #port: 25250
      #provider: dav
      director:
        user: director
        password: ((director_blobstore_password))
      agent:
        user: agent
        password: ((agent_blobstore_password))
    compiled_package_cache:
      provider: s3
      options:
        s3_region: ((aws_region))
        credentials_source: env_or_profile
        bucket_name: ((compiled_package_cache_bucket))
    default_ssh_options:
      gateway_host: ((director_dns))
    director:
      address: 127.0.0.1
      name: ((deployment_name))
      backup_destination:
        provider: s3
        options:
          s3_region: ((aws_region))
          credentials_source: env_or_profile
          bucket_name: ((director_backup_bucket))
      #generate_vm_passwords: true
      # https://bosh.io/docs/remove-dev-tools.html
      #remove_dev_tools: true
      db: &db
        host: ((bosh_db_dns))
        port: ((bosh_db_port))
        user: bosh
        password: ((bosh_db_password))
        database: bosh
        adapter: postgres
      cpi_job: aws_cpi
      enable_dedicated_status_worker: true
      enable_nats_delivered_templates: true
      enable_post_deploy: true
      enable_snapshots: true
      #snapshot_schedule: 0 0 7 * * * UTC
      #self_snapshot_schedule: 0 0 6 * * * UTC
      # To/from Director
      #encryption: true
      max_threads: 10
      ssl:
        key: ((director_ssl.private_key))
        cert: ((director_ssl.certificate))
      user_management:
        provider: local
        local:
          users:
          - {name: director, password: ((director_password))}
          - {name: hm, password: ((hm_password))}
    hm:
      director_account:
        user: hm
        password: ((hm_password))
        ca_cert: ((default_ca.certificate))
      resurrector_enabled: true
    nats:
      address: 127.0.0.1
      user: nats
      password: ((nats_password))
      tls:
        ca: ((nats_ca.certificate))
        server:
          certificate: ((nats_server_tls.certificate))
          private_key: ((nats_server_tls.private_key))
        client_ca:
          certificate: ((nats_ca.certificate))
          private_key: ((nats_ca.private_key))
        director:
          certificate: ((nats_clients_director_tls.certificate))
          private_key: ((nats_clients_director_tls.private_key))
        health_monitor:
          certificate: ((nats_clients_health_monitor_tls.certificate))
          private_key: ((nats_clients_health_monitor_tls.private_key))
    ntp: &ntp [0.pool.ntp.org, 1.pool.ntp.org]
    registry:
      host: ((bosh_lite_private_ip))
      db: *db
      http:
        user: admin
        password: ((http_password))
        port: 25777
      username: admin
      password: ((registry_password))
      port: 25777
    s3_config:
      bucket_name: ((backup_bucket))
      credentials_source: env_or_profile
      region: ((aws_region))
    postgresql_databases:
    - name: applications
      admin_username: ((apps_db_admin_username))
      admin_password: ((apps_db_admin_password))
      admin_database: postgres
      postgresql_host: ((apps_db_dns))
      postgresql_port: ((apps_db_port))
      rds: true
      databases:
      - { name: rds_broker, extensions: [pgcrypto], ignore_existing: true }
    - name: bosh
      admin_username: ((bosh_db_admin_username))
      admin_password: ((bosh_db_admin_password))
      admin_database: postgres
      postgresql_host: ((bosh_db_dns))
      postgresql_port: ((bosh_db_port))
      rds: true
      databases:
      # We don't want to backup the Bosh database.  IF we need to backup a CF instance, we can just backup the CF databases & blobs
      # then restore them with a new Bosh. We only use RDS for HA
      - { name: bosh, username: bosh, password: ((bosh_db_password)), extensions: [pgcrypto,citext], no_restore: true, drop_existing: true }
    - name: cloudfoundry
      admin_username: ((cf_db_admin_username))
      admin_password: ((cf_db_admin_password))
      admin_database: postgres
      postgresql_host: ((cf_db_dns))
      postgresql_port: ((cf_db_port))
      rds: true
      databases:
      - { name: ccdb, username: ccadmin, password: ((cc_db_password)), extensions: [pgcrypto,citext], ignore_existing: true }
      # The Diego DB only contains transient data, eg auction allocation status
      - { name: diego, username: diego, password: ((diego_db_password)), extensions: [pgcrypto,citext], no_restore: true, ignore_existing: true }
      - { name: uaadb, username: uaaadmin, password: ((uaa_db_password)), extensions: [pgcrypto,citext], ignore_existing: true }

cloud_provider:
  mbus: "https://mbus:((mbus_password))@((director_dns)):6868"
  cert: ((mbus_bootstrap_ssl))
  ssh_tunnel:
    host: ((director_dns))
    port: 22
    user: vcap
    private_key: ((bosh_ssh_key_file))
  template: {name: aws_cpi, release: bosh-aws-cpi}
  properties:
    agent: {mbus: "https://mbus:((mbus_password))@0.0.0.0:6868"}
    aws:
      access_key_id: ((bosh_aws_access_key_id))
      secret_access_key: ((bosh_aws_secret_access_key))
      default_key_name: ((bosh_ssh_key_name))
      region: ((aws_region))
      default_security_groups: [((director_instance_security_group))]
      # All VMs need to be able to write to the blob store to create logs!
      default_iam_instance_profile: ((blobstore_bucket_access_instance_profile))
    blobstore: {provider: local, path: /var/vcap/micro_bosh/data/cache}
    ntp: *ntp

addons:
variables:

